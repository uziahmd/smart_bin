# Smart Binarization: LLM Quantization Framework

A comprehensive framework for developing and evaluating smart binarization strategies for large language models. This project combines GPTQ-based quantization with quantization-aware training (QAT) to achieve extreme low-bit quantization while maintaining model performance.

## üéØ Project Overview

**Smart Binarization** implements partially-binarized LLM quantization, where a small ratio of salient weights are preserved in higher precision while the majority are binarized to extreme compression. The framework includes:

- ‚úÖ **Vanilla baseline evaluation** (unquantized reference)
- ‚úÖ **Smart Binarization quantized models** (current reference implementation)
- ‚úÖ **Extensible framework** for custom quantization algorithms
- ‚úÖ **Comprehensive evaluation tools** (perplexity, memory, speed)
- ‚úÖ **Automated testing and reporting**

## üìä Baseline Results (OPT-125M)

| Configuration | Perplexity | Memory | Speed | Status |
|---|---|---|---|---|
| **Vanilla** (baseline) | **28.62** | 250 MB | 396 tok/s | ‚úì Reference |
| **Smart Binarization** (80%) | 858.57 | 125 MB | ~400 tok/s | ‚úì Working |

**Note:** High PPL at 80% binarization is expected. Performance recovery through lower `low_frac`, hessian saliency, or QAT.

## üöÄ Quick Start

### Prerequisites
```bash
conda activate uzi  # Python 3.12 with all dependencies
```

### Evaluate Models
```bash
cd "/home/uzair/code/smart binarization"

# Vanilla model only
python evaluate_models.py --vanilla

# Generate quantized checkpoint and evaluate
python generate_quantized_checkpoint.py --nsamples 128
python evaluate_models.py --smart-bin

# Compare both models
python evaluate_models.py --compare

# Generate comprehensive comparison report
python final_comparison_report.py
```

### Run Full Quantization Pipeline
```bash
cd gptq_pb

# Standard parameters (80% binary, magnitude saliency)
python run.py facebook/opt-125m wikitext2 xnor \
  --nsamples 128 --low_frac 0.8 --high_bit 8 --salient_metric magnitude

# Better quality (50% binary, hessian saliency)
python run.py facebook/opt-125m wikitext2 xnor \
  --nsamples 128 --low_frac 0.5 --high_bit 8 --salient_metric hessian

# Try 2-bit quantization
python run.py facebook/opt-125m wikitext2 2bit \
  --nsamples 128 --low_frac 0.8 --high_bit 8 --salient_metric magnitude
```

## üìÅ Project Structure

```
smart binarization/
‚îú‚îÄ‚îÄ evaluate_models.py              Main evaluation framework
‚îú‚îÄ‚îÄ compare_models.py               Model comparison runner
‚îú‚îÄ‚îÄ final_comparison_report.py       Report generator
‚îú‚îÄ‚îÄ generate_quantized_checkpoint.py Checkpoint generator
‚îú‚îÄ‚îÄ download_datasets.py            Dataset pre-cacher
‚îú‚îÄ‚îÄ test_suite.py                   Automated test suite
‚îú‚îÄ‚îÄ requirements.txt                Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ gptq_pb/                        Smart Binarization implementation
‚îÇ   ‚îú‚îÄ‚îÄ run.py                      Main quantization script
‚îÇ   ‚îú‚îÄ‚îÄ gptq.py                     LowHighGPT algorithm
‚îÇ   ‚îú‚îÄ‚îÄ high_quant.py               High-bit quantizer
‚îÇ   ‚îú‚îÄ‚îÄ low_quant.py                Low-bit quantizer
‚îÇ   ‚îî‚îÄ‚îÄ outputs/mask/               Generated masks
‚îÇ
‚îú‚îÄ‚îÄ qat/                            Quantization-aware training
‚îÇ   ‚îú‚îÄ‚îÄ run_qat.py
‚îÇ   ‚îî‚îÄ‚îÄ eval_after_qat.py
‚îÇ
‚îú‚îÄ‚îÄ quant/                          Quantizer implementations
‚îÇ   ‚îú‚îÄ‚îÄ quantizer.py                Binary/STE layers
‚îÇ   ‚îî‚îÄ‚îÄ outlier_quantizer.py        Outlier-aware quantizers
‚îÇ
‚îú‚îÄ‚îÄ eval_results/                   Evaluation outputs
‚îÇ   ‚îú‚îÄ‚îÄ comparison_results_*.json
‚îÇ   ‚îú‚îÄ‚îÄ comparison_report_*.txt
‚îÇ   ‚îî‚îÄ‚îÄ final_comparison_*.txt
‚îÇ
‚îî‚îÄ‚îÄ cache/                          Local dataset cache
```

## üîß Framework Architecture

### Evaluation Pipeline
```
ModelEvaluator class:
  ‚îú‚îÄ‚îÄ load_vanilla_model()           Load unquantized model
  ‚îú‚îÄ‚îÄ load_smart_binarization()      Load quantized checkpoint
  ‚îú‚îÄ‚îÄ evaluate_perplexity()          Measure on wikitext2/wikitext-103/c4
  ‚îú‚îÄ‚îÄ evaluate_memory_usage()        GPU/CPU memory profiling
  ‚îî‚îÄ‚îÄ evaluate_inference_speed()     Tokens/second measurement
```

### Supported Models
- ‚úì facebook/opt-125m (tested, fast)
- ‚úì facebook/opt-1.3b (ready)
- ‚úì facebook/opt-6.7b (ready)
- ‚úì huggyllama/llama-7b (ready)

### Quantization Methods
- ‚úì `xnor` - Binary quantization (XNOR operation)
- ‚úì `sign` - Sign-based binary
- ‚úì `2bit` - 2-bit quantization
- ‚úì `4bit` - 4-bit quantization
- ‚úì `no` - No quantization (baseline)

### Saliency Metrics
- ‚úì `magnitude` - Weight magnitude ranking (fast)
- ‚úì `hessian` - Hessian-based saliency (better quality, slower)

### Datasets
- ‚úì **wikitext2** (cached) - 36K train, primary calibration
- ‚úì **wikitext-103-v1** (cached) - 1.8M train, thorough evaluation
- ‚úì **c4** (auto-download) - Large-scale pretraining corpus

## üí° How to Add Custom Algorithms

### 1. Create Quantizer
```python
# quant/my_algorithm.py
from torch import nn
from quant.quantizer import BinaryInterface

class MyQuantizer(nn.Module, BinaryInterface):
    def __init__(self, weight, bias):
        super().__init__()
        self.weight = nn.Parameter(weight.data)
        self.bias = nn.Parameter(bias.data) if bias else None
    
    def forward(self, x):
        w = self.quantize_weights()  # Your quantization logic
        return F.linear(x, w, self.bias)
    
    def get_save_weight_dict(self):
        return {"weight": self.weight.data.half().cpu(), "bias": self.bias}
```

### 2. Update Evaluator
```python
# evaluate_models.py - add to load_vanilla_model()
elif config['type'] == 'my_algorithm':
    self.model = load_my_algorithm(config['checkpoint'])
```

### 3. Run Evaluation
```bash
python evaluate_models.py --compare
python final_comparison_report.py
```

Results automatically save to `eval_results/` with JSON and formatted text reports.

## üìà Performance Optimization Guide

### Improve Quality from 858 PPL

1. **Lower binarization fraction**
   ```bash
   python run.py ... --low_frac 0.5  # Try 50% instead of 80%
   ```

2. **Better saliency detection**
   ```bash
   python run.py ... --salient_metric hessian  # vs magnitude
   ```

3. **Quantization-aware training**
   ```bash
   cd qat
   python run_qat.py facebook/opt-125m wikitext2 xnor
   ```

4. **Higher precision for salient weights**
   ```bash
   python run.py ... --high_bit 16  # vs 8-bit
   ```

5. **Multi-method comparison**
   - Test sign, 2bit, 4bit quantization
   - Compare results side-by-side

## üß™ Testing

### Quick Test (1 minute)
```bash
python test_suite.py --quick
```

### Standard Test (5 minutes)
```bash
python test_suite.py
```

### Thorough Test (30 minutes)
```bash
python test_suite.py --thorough
```

### Manual Verification
```bash
python evaluate_models.py --vanilla   # Should get PPL ‚âà 28.62
python evaluate_models.py --compare   # Compare both
python final_comparison_report.py     # Comprehensive summary
```

## üìö Key Scripts Reference

### evaluate_models.py
```bash
# Evaluate vanilla model
python evaluate_models.py --vanilla

# Evaluate smart binarization
python evaluate_models.py --smart-bin

# Compare both (default)
python evaluate_models.py --compare

# Options
--model TEXT              Model ID (default: facebook/opt-125m)
--dataset TEXT            Dataset (default: wikitext2)
```

### generate_quantized_checkpoint.py
```bash
python generate_quantized_checkpoint.py \
  --model facebook/opt-125m \
  --dataset wikitext2 \
  --nsamples 128 \
  --low-frac 0.8 \
  --high-bit 8 \
  --salient-metric magnitude
```

### compare_models.py
```bash
python compare_models.py \
  --model facebook/opt-125m \
  --dataset wikitext2
```

### final_comparison_report.py
```bash
python final_comparison_report.py
# Generates comprehensive comparison with insights and next steps
```

## üìã Development Roadmap

### Phase 1: Framework ‚úÖ
- [x] Vanilla baseline established (PPL: 28.62)
- [x] Smart Binarization working (PPL: 858.57 at 80%)
- [x] Evaluation infrastructure complete
- [x] Comparison framework ready

### Phase 2: Custom Algorithm (Next)
- [ ] Design algorithmic improvements
- [ ] Implement quantizer
- [ ] Test and compare vs baselines
- [ ] Optimize hyperparameters

### Phase 3: Scaling & Validation
- [ ] Test on larger models (opt-1.3b, opt-6.7b)
- [ ] Multi-method comparison (sign, 2bit, 4bit vs xnor)
- [ ] Performance benchmarking
- [ ] Paper preparation

## üîç Results & Artifacts

### Output Locations
- **Evaluation results**: `eval_results/`
  - `comparison_results_*.json` - Structured metrics (JSON)
  - `comparison_report_*.txt` - Formatted comparison
  - `final_comparison_*.txt` - Comprehensive summary with insights

- **Quantization outputs**: `gptq_pb/outputs/mask/`
  - Saved quantization masks for each layer

- **Cached datasets**: `~/.cache/huggingface/datasets/`
  - Preprocessed for fast loading

### Parse Results
```python
import json
with open('eval_results/comparison_results_*.json') as f:
    results = json.load(f)
    print(results['vanilla']['perplexity'])
    print(results['smart_binarization']['memory'])
```

## üíæ Environment & Dependencies

### Python Environment
```bash
conda activate uzi          # Python 3.12.12
pip install -r requirements.txt
```

### Key Dependencies
- **torch** >= 2.0.0
- **transformers** >= 4.30.0
- **datasets** >= 2.10.0
- **lm-eval** >= 0.4.0
- **bitsandbytes** >= 0.41.0

## ü§ù Contributing

To implement a new quantization algorithm:

1. Create quantizer class in `quant/`
2. Update `evaluate_models.py` with model type
3. Run evaluations with framework
4. Compare against baselines
5. Document methodology and results

## üìñ References

- **Original PB-LLM Paper**: [arxiv.org/abs/2310.00034](https://arxiv.org/abs/2310.00034)
  - Authors: Yuzhang Shang, Zhihang Yuan, Qiang Wu, Zhen Dong
  - Partially-Binarized LLMs via post-training quantization (GPTQ) and QAT

- **GPTQ Method**: General-Purpose Quantization for Large Language Models
- **Quantization-aware Training (QAT)**: For performance recovery

## ‚ú® Status

**Framework is complete and ready for algorithm development!**

- All baseline metrics locked
- Evaluation infrastructure validated
- Datasets cached for reproducibility
- Ready to implement and test custom algorithms

---

**Last Updated:** January 14, 2026  
**Project Status:** üü¢ Ready for development
